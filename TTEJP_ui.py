"""
JRec-IN Portalçˆ¬è™« - Streamlitç•Œé¢
ç”¨äºæ§åˆ¶çˆ¬è™«å‚æ•°ã€å¯åŠ¨çˆ¬å–è¿‡ç¨‹å¹¶æŸ¥çœ‹ç»“æœ
"""

import streamlit as st
import pandas as pd
import json
import os
import sys
import time
from datetime import datetime

# import subprocess
# import importlib.util
# from jrecin_analyzer import *
# from jrecin_scraper import *
# from jrecin_llm_analyzer import *

# å¯¼å…¥çˆ¬è™«æ¨¡å—
try:
    # ç¡®ä¿çˆ¬è™«æ¨¡å—å¯ä»¥å¯¼å…¥
    sys.path.append(os.path.abspath('.'))
    from jrecin_scraper import main as run_scraper
except ImportError:
    st.error("Unable to import the scraper module, please ensure that jrecin_scraper.py is in the same directory.")

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="TenureTrack Explorer(JP)",
    page_icon="ğŸ”",
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ ‡é¢˜å’Œä»‹ç»
st.title("TenureTrack Explorer(JP)")
st.markdown(
    "[![Open in GitHub](https://img.shields.io/badge/GitHub-181717?style=flat&logo=github&logoColor=white)](https://github.com/ShuXingYu94/TenureTrackExplorerJP)&nbsp;&nbsp;  [![Built with streamlit](https://img.shields.io/badge/-Streamlit-FF4B4B?style=flat&logo=streamlit&logoColor=white)](https://streamlit.io/)&nbsp;&nbsp;  [![Static Badge](https://img.shields.io/badge/JREC--IN-PORTAL-test?logoSize=3&labelColor=grey&color=white)](https://jrecin.jst.go.jp/seek/SeekTop)")
st.markdown("")

st.markdown("""
#### Scrape job information from the **JRec-IN** Portal website""")

# ä¾§è¾¹æ  - å‚æ•°è®¾ç½®
st.sidebar.header("Parameters")

# è¿è¡Œæ¨¡å¼é€‰æ‹©
run_mode = st.sidebar.radio(
    "Mode",
    ["Collect URL only", "Process details only", "Full workflow"],
    index=0
)

# å°†æ˜¾ç¤ºåç§°è½¬æ¢ä¸ºç¨‹åºä¸­çš„æ¨¡å¼å€¼
mode_map = {
    "Collect URL only": "urls_only",
    "Process details only": "details_only",
    "Full workflow": "full"
}

# å…³é”®è¯è®¾ç½®
default_keywords = "ç†è«–çµŒæ¸ˆå­¦ çµŒæ¸ˆå­¦èª¬ çµŒæ¸ˆæ€æƒ³ çµŒæ¸ˆæ”¿ç­–"
keywords = st.sidebar.text_area("Keywords (with space in between)", default_keywords)

# æœ€å¤§é¡µæ•°è®¾ç½®
max_pages = st.sidebar.slider("Max crawling pages", 1, 30, 10)

# å¦‚æœé€‰æ‹©äº†å¤„ç†è¯¦æƒ…é¡µé¢ï¼Œæ˜¾ç¤ºæœ€å¤§å¤„ç†èŒä½æ•°é‡é€‰é¡¹
if run_mode != "Collect URL only":
    max_jobs = st.sidebar.slider("Max jobs to process", 1, 100, 10,
                                 help="Limit the number of jobs to process. None means process all jobs")
    use_all_jobs = st.sidebar.checkbox("Process all jobs", value=False)
else:
    max_jobs = 10
    use_all_jobs = False

# æµ‹è¯•æ¨¡å¼é€‰é¡¹
test_mode = st.sidebar.checkbox("Test mode", value=False,
                                help="Enable test mode to save more intermediate files for debugging.")

execution = st.sidebar.button("Run")

# ä¸»ç•Œé¢

# æ˜¾ç¤ºå½“å‰çŠ¶æ€
status_placeholder = st.empty()
status_placeholder.info("Please click the Run button to start scraping.")

# è¿è¡ŒæŒ‰é’®
if execution:
    # å‡†å¤‡è¿è¡Œå‚æ•°
    mode = mode_map[run_mode]
    jobs_param = None if use_all_jobs else max_jobs

    # æ›´æ–°çŠ¶æ€
    status_placeholder.info(f"Running on: {run_mode}, Keywords: {keywords}, Max pages: {max_pages}")

    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    try:
        # è¿è¡Œçˆ¬è™«
        run_scraper(max_pages=max_pages,
                    max_jobs=jobs_param,
                    keywords=keywords,
                    mode=mode,
                    test_optimal=test_mode)

        # è®¡ç®—è¿è¡Œæ—¶é—´
        end_time = time.time()
        elapsed_time = end_time - start_time

        # æ›´æ–°çŠ¶æ€
        status_placeholder.success(f"Complete! Time consumed: {elapsed_time:.2f} s")

    except Exception as e:
        status_placeholder.error(f"Error occurred: {str(e)}")

# çˆ¬å–ç»“æœæŸ¥çœ‹
st.header("Result")

# New
st.markdown("#### List of new positions")

# æ£€æŸ¥URLåˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
all_urls_file = 'jrecin_data/all_job_urls.json'
new_urls_file = 'jrecin_data/new_job_urls.json'

if os.path.exists(new_urls_file):
    try:
        with open(new_urls_file, 'r', encoding='utf-8-sig') as f:
            all_urls = json.load(f)

        # æ˜¾ç¤ºURLæ€»æ•°
        st.info(f"A total of {len(all_urls)} position detected.")

        # å°†URLåˆ—è¡¨è½¬æ¢ä¸ºDataFrameå¹¶æ˜¾ç¤º
        url_df = pd.DataFrame(all_urls)
        st.dataframe(
            url_df,
            column_config={
                "url": st.column_config.LinkColumn(
                    "Link"
                )
            },
            hide_index=True,
        )

    except Exception as e:
        st.error(f"Error while trying to access file: {str(e)}")
else:
    st.warning("URL list file does not exist, please run the crawler to collect URLs first.")

# All
st.markdown("#### List of all positions")

# æ£€æŸ¥URLåˆ—è¡¨æ–‡ä»¶æ˜¯å¦å­˜åœ¨
all_urls_file = 'jrecin_data/all_job_urls.json'
new_urls_file = 'jrecin_data/new_job_urls.json'

if os.path.exists(all_urls_file):
    try:
        with open(all_urls_file, 'r', encoding='utf-8-sig') as f:
            all_urls = json.load(f)

        # æ˜¾ç¤ºURLæ€»æ•°
        st.info(f"A total of {len(all_urls)} position detected.")

        # å°†URLåˆ—è¡¨è½¬æ¢ä¸ºDataFrameå¹¶æ˜¾ç¤º
        url_df = pd.DataFrame(all_urls)

        st.dataframe(
            url_df,
            column_config={
                "url": st.column_config.LinkColumn(
                    "Link"
                )
            },
            hide_index=True,
        )

    except Exception as e:
        st.error(f"Error while trying to access file: {str(e)}")
else:
    st.warning("URL list file does not exist, please run the crawler to collect URLs first.")

# åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown(f"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
